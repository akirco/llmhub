# LLMHub - Rust Client for Large Language Models

A Rust-powered client library for interacting with large language models (LLMs). Supports streaming and non-streaming conversations with various API providers.

## Notice

This is a work in progress and is not yet ready for production use.some providers are maybe supported yet.

## Models supported

`checkout /src/models/models.rs for more details.`

## Providers supported

`checkout /src/api/providers.rs for more details.`


## Features âœ¨

- ğŸš€ Async-first implementation using Tokio runtime
- ğŸŒ Multi-provider support (Deepseek, etc.)
- ğŸ“¡ Stream response handling with backpressure support
- ğŸ”§ Configurable API endpoints and rate limiting
- ğŸ› ï¸ Strong type safety with Rust enums and structs
- ğŸ§  Conversation memory management
- ğŸš¦ Comprehensive error handling

## Installation ğŸ“¦

Add to your `Cargo.toml`:

```toml
[dependencies]
llmhub = { git = "https://github.com/akirco/llmhub" }
```

## Usage ğŸš€

`checkout examples`

```rust
cargo run --example llmhub_test
```

## Contributing ğŸ¤

Feel free to open issues or pull requests.

## License ğŸ“„

MIT License